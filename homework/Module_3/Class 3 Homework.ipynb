{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Assignment: Voice Agent Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we start to hands on buiding Research Voice Agent, truly useful AI Research Assistants must listen, understand, and respond with voice. **we will give you some simple introduction code as a starter, feel free to write your own code or do optimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Learning Objectives this week\n",
    "to build a simple Voice Agent, we need these following knowledge.\n",
    "\n",
    "* **1. Speech Recognition (ASR):** Convert audio to text using models like Whisper or Google Speech-to-Text.\n",
    "* **2. Dialogue Generation with LLMs:** Feed transcribed user input into LLM (e.g. LLaMA 3) and generate natural language responses.\n",
    "* **3. Text-to-Speech (TTS):** Use a TTS engine (CozyVoice) to convert generated responses into spoken audio.\n",
    "* **4. FastAPI for API Serving:** Create a web server with FastAPI to handle audio file uploads and return voice responses.\n",
    "* **5. Conversation State Management:** Track conversation history to enable multi-turn interaction.\n",
    "* **6. Low-Latency Real-Time Processing:** Use asynchronous functions to reduce inference time and improve response experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> âœ… You do NOT need Docker. Just ensure your local Python environment works.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Project: Build an Local Voice Assistant\n",
    "\n",
    "### ðŸŽ¯ Goal:\n",
    "\n",
    "Develop a real-time voice chatbot that can:\n",
    "\n",
    "1. Take audio input via HTTP,\n",
    "2. Transcribe audio to text (ASR),\n",
    "3. Generate a response using LLM,\n",
    "4. Convert the response back to speech (TTS),\n",
    "5. Support 5-turn conversational memory.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: FastAPI Skeleton\n",
    "\n",
    "Create a simple FastAPI server that accepts an audio file via POST and returns an audio file in response:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the official guidance of FastAPI [fastapi](https://fastapi.tiangolo.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    audio_bytes = await file.read()\n",
    "    # TODO: ASR â†’ LLM â†’ TTS\n",
    "    return FileResponse(\"response.wav\", media_type=\"audio/wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your server:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Test it with `curl`, Postman, or a custom frontend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: ASR (Speech Recognition)\n",
    "\n",
    "Use OpenAI Whisper to transcribe the uploaded audio to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "asr_model = whisper.load_model(\"small\")\n",
    "\n",
    "def transcribe_audio(audio_bytes):\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(audio_bytes)\n",
    "    result = asr_model.transcribe(\"temp.wav\")\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it to the `/chat/` route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = transcribe_audio(audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `user_text` for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Response Generation (LLM)\n",
    "\n",
    "Generate context-aware responses using Llama 3. Use HuggingFace `pipeline` to call LLaMA 3 or similar models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=\"meta-llama/Llama-3-8B\")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(user_text):\n",
    "    conversation_history.append({\"role\": \"user\", \"text\": user_text})\n",
    "    # Construct prompt from history\n",
    "    prompt = \"\"\n",
    "    for turn in conversation_history[-5:]:\n",
    "        prompt += f\"{turn['role']}: {turn['text']}\\n\"\n",
    "    outputs = llm(prompt, max_new_tokens=100)\n",
    "    bot_response = outputs[0][\"generated_text\"]\n",
    "    conversation_history.append({\"role\": \"assistant\", \"text\": bot_response})\n",
    "    return bot_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call in route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_text = generate_response(user_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: TTS (Text to Speech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert LLM text responses to natural-sounding speech. \\\n",
    "try to use cozyvoice to complete Text to Speech, here is the original project.\n",
    "[Cozyvoice](https://github.com/FunAudioLLM/CosyVoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cozyvoice import CozyVoice\n",
    "\n",
    "tts_engine = CozyVoice()\n",
    "\n",
    "def synthesize_speech(text, filename=\"response.wav\"):\n",
    "    tts_engine.generate(text, output_file=filename)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it in the route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonaudio_path = synthesize_speech(bot_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Full Integration\n",
    "\n",
    "Your final `/chat/` endpoint might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    audio_bytes = await file.read()\n",
    "    user_text = transcribe_audio(audio_bytes)\n",
    "    bot_text = generate_response(user_text)\n",
    "    audio_path = synthesize_speech(bot_text)\n",
    "    return FileResponse(audio_path, media_type=\"audio/wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… Deliverables\n",
    "\n",
    "* [ ] A runnable FastAPI project with `/chat/` endpoint\n",
    "* [ ] A working voice assistant that handles **5-turn** multi-round conversations\n",
    "* [ ] Code with clear structure and modular components (ASR, LLM, TTS)\n",
    "* [ ] A **2-minute screen recording** demo: record 5 turns of real-time interaction\n",
    "* [ ] Optional: Add conversation memory display, prompt formatting logic, async optimization\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒŸ Extension Ideas (Optional)\n",
    "\n",
    "* Use `async` processing for parallel ASR/LLM/TTS.\n",
    "* Integrate a microphone frontend UI for live recording.\n",
    "* Add speaker identification or personalized voice response.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
