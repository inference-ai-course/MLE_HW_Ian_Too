import os
import logging
import requests
from dotenv import load_dotenv
from transformers import pipeline

# Initialize
load_dotenv()
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s")

# conversation historu, 
conversation_history = []

# Define system prompt once
system_prompt = "system: You are a helpful assistant who answers clearly and concisely.\n"

def generate_response(user_text: str):
    """
    Generate text response using huggingface.
    """

    # Access the token
    token = os.getenv("HF_TOKEN")
    logging.debug(f"Token: {token}")

    model_id = "microsoft/phi-3-mini-128k-instruct"
    logging.debug(f"Model: {model_id}")

    # choose the llm
    llm = pipeline("text-generation", model=model_id)

    logging.debug(f"Received user text: {user_text}\n")

    logging.debug(f"User text: {user_text}")
    
    # add a user conversation history for context.
    conversation_history.append({
        "role":"user",
        "content": user_text 
    })

    # construct the prompt from history.
    prompt = system_prompt + """
    user: Hello there, what is your name?
    assistant: Hello! My name is Echo. It's nice to meet you. How can I help you today?
    """

    # build from the last 5 conversation
    for message in conversation_history[-5:]:
        prompt += f"{message['role']} : {message['content']}\n"

    # pass it ot the llm and get feedback
    logging.debug("Getting response...")
    output = llm(prompt, max_new_tokens=100,  return_full_text=False)
    logging.debug("Done")

    llm_response = output[0]["generated_text"] # get the text generated by llm

    # add to the conversational history
    conversation_history.append({
        "role":"assistant",
        "content": llm_response
    })

    logging.debug(f"llm response: {llm_response}")

    return llm_response

ollama_model_name="smollm:1.7b"
def generate_response_ollama(user_text: str, model_name=ollama_model_name):
    """
    Generate text response using ollama.
    """
    logging.debug(f"Ollama user text: {user_text}")

    # Add to conversation history
    conversation_history.append({
        "role": "user",
        "content": user_text
    })

    # Build prompt with system message
    prompt = system_prompt
    for message in conversation_history[-5:]:
        prompt += f"{message['role']}: {message['content']}\n"

    # Send to Ollama
    logging.debug("Sending request to Ollama...")
    response = requests.post(
        "http://127.0.0.1:11434/api/generate",
        json={
            "model": model_name,
            "prompt": prompt,
            "stream": False
        }
    )

    if response.status_code != 200:
        logging.error(f"Ollama error: {response.status_code} - {response.text}")
        return "Error: Failed to get response from Ollama."

    llm_response = response.json()["response"]
    logging.debug(f"Ollama response: {llm_response}")

    # Add assistant response to history
    conversation_history.append({
        "role": "assistant",
        "content": llm_response
    })

    return llm_response

def generate_response_claude(user_text: str, model_name="claude-3-sonnet-20240229"):
    """
    Using claude sonnet.
    """
    logging.debug(f"Claude user text: {user_text}")

    # Add to conversation history
    conversation_history.append({
        "role": "user",
        "content": user_text
    })

    # Build prompt with system message
    prompt = system_prompt
    for message in conversation_history[-5:]:
        prompt += f"{message['role']}: {message['content']}\n"

    # Prepare Claude-style messages
    messages = [
        {"role": "system", "content": "Your name is Como AI. You are a helpful assistant who answers clearly and concisely."},
        {"role": "user", "content": prompt}
    ]

    # Send request to Claude
    response = requests.post(
        "https://api.anthropic.com/v1/messages",
        headers={
            "x-api-key": os.getenv("ANTHROPIC_API_KEY"),
            "anthropic-version": "2023-06-01"
        },
        json={
            "model": model_name,
            "max_tokens": 10000,
            "messages": messages
        }
    )

    if response.status_code != 200:
        logging.error(f"Claude error: {response.status_code} - {response.text}")
        return "Error: Failed to get response from Claude."

    llm_response = response.json()["content"]
    logging.debug(f"Claude response: {llm_response}")

    # Add assistant response to history
    conversation_history.append({
        "role": "assistant",
        "content": llm_response
    })

    return llm_response
