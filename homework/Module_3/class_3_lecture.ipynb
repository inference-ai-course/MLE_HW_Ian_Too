{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/python_autocite-0.0.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: trafilatura in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (2.0.0)\n",
      "Requirement already satisfied: requests in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: bs4 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: fitz in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: pytesseract in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (10.4.0)\n",
      "Requirement already satisfied: surya-ocr in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (0.10.3)\n",
      "Requirement already satisfied: certifi in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from trafilatura) (2024.8.30)\n",
      "Requirement already satisfied: charset_normalizer>=3.4.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from trafilatura) (3.4.0)\n",
      "Requirement already satisfied: courlan>=1.3.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from trafilatura) (1.3.2)\n",
      "Requirement already satisfied: htmldate>=1.9.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from trafilatura) (1.9.3)\n",
      "Requirement already satisfied: justext>=3.0.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from trafilatura) (3.0.2)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from trafilatura) (5.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from trafilatura) (1.26.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: configobj in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (5.0.9)\n",
      "Requirement already satisfied: configparser in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (7.2.0)\n",
      "Requirement already satisfied: httplib2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: nibabel in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (5.3.2)\n",
      "Requirement already satisfied: nipype in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (1.10.0)\n",
      "Requirement already satisfied: numpy in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (2.0.3)\n",
      "Requirement already satisfied: pyxnat in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (1.6.3)\n",
      "Requirement already satisfied: scipy in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from fitz) (1.14.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.8 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (8.1.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (1.2.0)\n",
      "Requirement already satisfied: opencv-python<5.0.0.0,>=4.9.0.80 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (4.10.0.84)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.3 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (2.10.6)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.1.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (2.7.1)\n",
      "Requirement already satisfied: pypdfium2==4.30.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (4.30.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (1.0.0)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.5.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (2.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from surya-ocr) (4.46.1)\n",
      "Requirement already satisfied: babel>=2.16.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from courlan>=1.3.2->trafilatura) (2.16.0)\n",
      "Requirement already satisfied: tld>=0.13 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from courlan>=1.3.2->trafilatura) (0.13)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from htmldate>=1.9.2->trafilatura) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.3->surya-ocr) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.3->surya-ocr) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.5.3->surya-ocr) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch<3.0.0,>=2.5.1->surya-ocr) (3.15.4)\n",
      "Requirement already satisfied: networkx in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch<3.0.0,>=2.5.1->surya-ocr) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch<3.0.0,>=2.5.1->surya-ocr) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch<3.0.0,>=2.5.1->surya-ocr) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch<3.0.0,>=2.5.1->surya-ocr) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sympy==1.13.1->torch<3.0.0,>=2.5.1->surya-ocr) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->surya-ocr) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->surya-ocr) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->surya-ocr) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->surya-ocr) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->surya-ocr) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->surya-ocr) (4.66.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from httplib2->fitz) (3.1.2)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nibabel->fitz) (6.5.2)\n",
      "Requirement already satisfied: prov>=1.5.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (2.0.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (3.0.4)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (3.20.1)\n",
      "Requirement already satisfied: traits>=6.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (7.0.2)\n",
      "Requirement already satisfied: acres in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (1.3.0)\n",
      "Requirement already satisfied: puremagic in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nipype->fitz) (1.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: pathlib>=1.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: tzlocal>=0.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.2)\n",
      "Requirement already satisfied: ci-info>=0.2 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: lxml-html-clean in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.16.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from jinja2->torch<3.0.0,>=2.5.1->surya-ocr) (2.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install trafilatura requests bs4 fitz pytesseract pillow surya-ocr faster-whisper openai-whisper datasketch\n",
    "\n",
    "# install ffmpg for Whisper to process your audio\n",
    "# On macOS (with Homebrew)\n",
    "! brew install ffmpeg\n",
    "#On Ubuntu/Debian:\n",
    "# ! sudo apt-get update -y\n",
    "# ! sudo apt-get install -y ffmpeg\n",
    "# 👉 On Windows (if using WSL or native):\n",
    "# You can download it from:\n",
    "# 🔗 https://ffmpeg.org/download.html\n",
    "# Or use a package manager like Chocolatey:\n",
    "# ! choco install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Pretraining Data Collection & Extraction - Hands-on Notebook\n",
    "\n",
    "## 1. Clean Web Page Text Using trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Extracted Text Preview:\n",
      "\n",
      "Physics > Physics Education\n",
      "[Submitted on 3 Feb 2024]\n",
      "Title:Uso de herramientas digitales matemáticas en la Educación Secundaria\n",
      "View PDF HTML (experimental)Abstract:Information and Community Technologies (ICT) are very present in our society nowadays and particularly in the educative field. In just two decades, we have passed from a learning based, in many cases, on the master lessons to one such that methodologies like the flipped classroom or the gamification are stronger than ever. Along this work, we have done a study to teachers and students with the main objective to compare the knowledge on digital tools, their use and their acceptation. We use WxMaxima and Geogebra in order to solve an exercise of \\textit{Evaluación de Bachillerato para el Acceso a la Universidad} (EBAU) related with Geometry, comparing their ins and outs with the manual solution. Finally, we expose some conclusions and some possible research lines about digital tools, as well as a proposition of an introducto\n"
     ]
    }
   ],
   "source": [
    "# ✅ Install dependencies if not already installed\n",
    "# !pip install trafilatura\n",
    "\n",
    "import trafilatura\n",
    "import requests\n",
    "\n",
    "# Example: An arXiv paper abstract page\n",
    "url = \"https://arxiv.org/abs/2404.00001\"\n",
    "\n",
    "# Step 1: Fetch raw HTML\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Step 2: Use Trafilatura to extract clean text\n",
    "downloaded_text = trafilatura.extract(html, include_comments=False, include_tables=False)\n",
    "\n",
    "# Step 3: Display the result\n",
    "print(\"📄 Extracted Text Preview:\\n\")\n",
    "print(downloaded_text[:1000])  # Show first 1000 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "trafilatura.extract() pulls main article content while removing headers, menus, and boilerplate.\n",
    "\n",
    "This works great on academic websites like arXiv, blog posts, or news articles.\n",
    "\n",
    "No need to write custom HTML parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: OCR – Convert Images to Text\n",
    "### Option A: Tesseract OCR (Offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might use the following install if the pytesseract is not installed\n",
    "# ! sudo apt-get update -y\n",
    "# ! sudo apt-get install -y tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Tesseract OCR Output (first 500 chars):\n",
      "a 4: Leading intelligence.\n",
      "\n",
      "Unrivaled speed and efficiency.\n",
      "\n",
      "The most accessible and scalable generation of Llama is here.\n",
      "Native multimodality, mixture-of-experts models, super long\n",
      "context windows, step changes in performance, and\n",
      "unparalleled efficiency. All in easy-to-deploy sizes custom fit for\n",
      "how you want to use it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install: sudo apt install tesseract-ocr OR !pip install pytesseract Pillow\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Load and preprocess image (convert to grayscale)\n",
    "image = Image.open(\"./test_data/image/image.png\").convert(\"L\")  # grayscale\n",
    "text = pytesseract.image_to_string(image)\n",
    "\n",
    "print(\"📄 Tesseract OCR Output (first 500 chars):\")\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Surya OCR (Fast PyTorch-based layout-aware tool)\n",
    "https://github.com/VikParuchuri/surya\n",
    "\n",
    "### Usage\n",
    "To perform OCR on an image, PDF, or a folder containing them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Good for: simple single-column text, PDFs converted to images\n",
    "* Struggles with layout, math, or low-res scans \n",
    "    * As you can see from the image: \"Download Models\" has not been extreact out correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model vikp/surya_det3 on device mps with dtype torch.float16\n",
      "Loaded recognition model vikp/surya_rec2 on device mps with dtype torch.float16\n",
      "Detecting bboxes: 100%|███████████████████████████| 1/1 [00:02<00:00,  2.02s/it]\n",
      "Recognizing Text: 100%|███████████████████████████| 1/1 [00:11<00:00, 11.26s/it]\n",
      "Wrote results to /Users/scottlai/Library/Mobile Documents/com~apple~CloudDocs/Desktop/work/inferenceAI/Class3/results/image\n"
     ]
    }
   ],
   "source": [
    "! surya_ocr ./test_data/image/image.png --langs en --images --output_dir results/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "**DATA_PATH** is the path to your image, PDF, or folder.\n",
    "\n",
    "**--langs** specifies the language(s) for OCR (e.g., en for English).\n",
    "\n",
    "**--images** saves images of the pages and detected text lines (optional).\n",
    "\n",
    "**--output_dir** specifies the directory to save results.​\n",
    "\n",
    "This command will generate a results.json file containing the detected text and bounding boxes.​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output Structure\n",
    "The **results.json** will have entries like:​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"image\": [\n",
    "    {\n",
    "      \"text_lines\": [\n",
    "        {\n",
    "          \"polygon\": [\n",
    "            [\n",
    "              13,\n",
    "              48\n",
    "            ],\n",
    "            [\n",
    "              538,\n",
    "              51\n",
    "            ],\n",
    "            [\n",
    "              538,\n",
    "              87\n",
    "            ],\n",
    "            [\n",
    "              12,\n",
    "              84\n",
    "            ]\n",
    "          ],\n",
    "          \"confidence\": 0.9970703125,\n",
    "          \"text\": \"Llama 4: Leading intelligence.\",\n",
    "          \"bbox\": [\n",
    "            12,\n",
    "            48,\n",
    "            538,\n",
    "            87\n",
    "          ]\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "          \"polygon\": [\n",
    "            [\n",
    "              47,\n",
    "              364\n",
    "            ],\n",
    "            [\n",
    "              176,\n",
    "              364\n",
    "            ],\n",
    "            [\n",
    "              176,\n",
    "              378\n",
    "            ],\n",
    "            [\n",
    "              47,\n",
    "              378\n",
    "            ]\n",
    "          ],\n",
    "          \"confidence\": 0.9716796875,\n",
    "          \"text\": \"Download models\",\n",
    "          \"bbox\": [\n",
    "            47,\n",
    "            364,\n",
    "            176,\n",
    "            378\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"languages\": [\n",
    "        \"en\"\n",
    "      ],\n",
    "      \"image_bbox\": [\n",
    "        0,\n",
    "        0,\n",
    "        600,\n",
    "        471\n",
    "      ],\n",
    "      \"page\": 1\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### or in python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model vikp/surya_det3 on device mps with dtype torch.float16\n",
      "Loaded recognition model vikp/surya_rec2 on device mps with dtype torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "Recognizing Text: 100%|██████████| 1/1 [00:03<00:00,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Llama 4: Leading intelligence.\n",
      "Confidence: 0.9970703125\n",
      "Polygon: [[13.0, 48.0], [538.0, 51.0], [538.0, 87.0], [12.0, 84.0]]\n",
      "\n",
      "Text: Unrivaled speed and efficiency.\n",
      "Confidence: 0.99462890625\n",
      "Polygon: [[12.0, 116.0], [564.0, 113.0], [565.0, 148.0], [13.0, 151.0]]\n",
      "\n",
      "Text: The most accessible and scalable generation of Llama is here.\n",
      "Confidence: 0.9990234375\n",
      "Polygon: [[13.0, 186.0], [565.0, 186.0], [565.0, 204.0], [13.0, 204.0]]\n",
      "\n",
      "Text: Native multimodality, mixture-of-experts models, super long\n",
      "Confidence: 0.99462890625\n",
      "Polygon: [[12.0, 214.0], [557.0, 212.0], [558.0, 230.0], [13.0, 231.0]]\n",
      "\n",
      "Text: context windows, step changes in performance, and\n",
      "Confidence: 0.99853515625\n",
      "Polygon: [[13.0, 240.0], [481.0, 240.0], [481.0, 258.0], [13.0, 258.0]]\n",
      "\n",
      "Text: unparalleled efficiency. All in easy-to-deploy sizes custom fit for\n",
      "Confidence: 0.9990234375\n",
      "Polygon: [[13.0, 268.0], [586.0, 268.0], [586.0, 285.0], [13.0, 285.0]]\n",
      "\n",
      "Text: how you want to use it.\n",
      "Confidence: 0.9765625\n",
      "Polygon: [[13.0, 295.0], [224.0, 295.0], [224.0, 312.0], [13.0, 312.0]]\n",
      "\n",
      "Text: Download models\n",
      "Confidence: 0.9716796875\n",
      "Polygon: [[47.0, 364.0], [176.0, 364.0], [176.0, 378.0], [47.0, 378.0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from surya.detection import DetectionPredictor\n",
    "from surya.recognition import RecognitionPredictor\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(\"./test_data/image/image.png\")  # Replace with your image path\n",
    "langs = [\"en\"]  # Specify the language(s)\n",
    "\n",
    "# Initialize predictors\n",
    "detection_predictor = DetectionPredictor()\n",
    "recognition_predictor = RecognitionPredictor()\n",
    "\n",
    "# Perform OCR\n",
    "predictions = recognition_predictor([image], [langs], detection_predictor)\n",
    "\n",
    "# Display results with polygon coordinates\n",
    "for page in predictions:\n",
    "    for line in page.text_lines:\n",
    "        print(f\"Text: {line.text}\")\n",
    "        print(f\"Confidence: {line.confidence}\")\n",
    "        print(f\"Polygon: {line.polygon}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Good for: structured layouts like academic papers\n",
    "* Fast inference and easy to integrate with PDF workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: OpenAI GPT-4o Vision OCR (Highly Accurate & Multicolumn)\n",
    "don't forget to add you `OPENAI_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Llama 4: Leading intelligence. Unrivaled speed and efficiency.**\n",
      "\n",
      "The most accessible and scalable generation of Llama is here. Native multimodality, mixture-of-experts models, super long context windows, step changes in performance, and unparalleled efficiency. All in easy-to-deploy sizes custom fit for how you want to use it.\n",
      "\n",
      "**Download models**\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def vision_extract(b64_image, prompt, api_key):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_image}\"}}\n",
    "            ]}\n",
    "        ],\n",
    "        \"max_tokens\": 3000\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# Load image and run GPT-4o OCR\n",
    "with open(\"test_data/image/image.png\", \"rb\") as f:\n",
    "    b64_img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# Use your actual API key here\n",
    "result = vision_extract(b64_img, \"Extract all the readable text from this document.\", api_key=\"YOUR_OPENAI_API_KEY\")\n",
    "print(result[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Good for: complex, multi-column documents and natural layout reasoning\n",
    "* Great fallback when you need accuracy over speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Speech Recognition (ASR)\n",
    "### Option A: Whisper by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! brew install ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Whisper Transcription:\n",
      " So we pay for that, they didn't, you know, there are some people, they said, it's on, Kren the Benzo, instead of caught in a musical, I was just not talking about your own interest and just turning child, that's the...\n"
     ]
    }
   ],
   "source": [
    "# Install: pip install openai-whisper\n",
    "import whisper\n",
    "\n",
    "# Load model\n",
    "model = whisper.load_model(\"base\")  # or \"small\", \"medium\", \"large\"\n",
    "\n",
    "# Transcribe audio\n",
    "result = model.transcribe(\"./test_data/audio/sample-1.mp3\")\n",
    "print(\"📄 Whisper Transcription:\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Great for: balanced speed and accuracy\n",
    "* Supports many audio formats: mp3, wav, m4a, webm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Faster-Whisper (Fast & Lightweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install faster-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Faster-Whisper Transcription:\n",
      "[0.00 - 4.00]  If we pay for that, they don't, you know, there are some people who say that it's on.\n",
      "[4.00 - 8.00]  Cren the Benzo instead of caught in a musical or there's not a story of what you're interested in.\n",
      "[8.00 - 10.00]  It's just turning child, that's the...\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Load model with float16 for speed\n",
    "model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"int8\")  # For CPUs\n",
    "\n",
    "# Transcribe\n",
    "segments, _ = model.transcribe(\"./test_data/audio/sample-1.mp3\")\n",
    "\n",
    "print(\"📄 Faster-Whisper Transcription:\")\n",
    "for segment in segments:\n",
    "    print(f\"[{segment.start:.2f} - {segment.end:.2f}] {segment.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Optimized for GPU or even CPU \n",
    "* Useful when batch-processing long audio datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pretraining Data Cleaning Pipeline\n",
    "### Step 1: Remove duplicates using MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def minhash_deduplication(texts, threshold=0.7):\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n",
    "    unique_texts = []\n",
    "    for i, doc in enumerate(texts):\n",
    "        m = MinHash(num_perm=128)\n",
    "        for word in set(doc.split()):\n",
    "            m.update(word.encode('utf8'))\n",
    "        if not lsh.query(m):\n",
    "            lsh.insert(f\"doc{i}\", m)\n",
    "            unique_texts.append(doc)\n",
    "    return unique_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Filter for language and strip HTML noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/python_autocite-0.0.4-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langdetect in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Users/scottlai/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html_and_filter_lang(texts, lang='en'):\n",
    "    filtered = []\n",
    "    for txt in texts:\n",
    "        txt = BeautifulSoup(txt, 'html.parser').get_text()\n",
    "        try:\n",
    "            if detect(txt.strip()) == lang:\n",
    "                filtered.append(txt.strip())\n",
    "        except:\n",
    "            continue\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Strip PII using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strip_pii(text):\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '[EMAIL]', text)\n",
    "    text = re.sub(r'\\b\\d{12,19}\\b', '[CREDIT_CARD]', text)\n",
    "    text = re.sub(r'\\b(?:\\d{3}-){2}\\d{4}\\b', '[PHONE]', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Remove repetitive n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def remove_repetitive_ngrams(text, n=3, threshold=3):\n",
    "    words = text.split()\n",
    "    ngrams = [' '.join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "    counts = Counter(ngrams)\n",
    "    repetitive = [ngram for ngram, count in counts.items() if count >= threshold]\n",
    "\n",
    "    for phrase in repetitive:\n",
    "        # regex-safe version of the phrase\n",
    "        escaped_phrase = re.escape(phrase)\n",
    "        # match the phrase repeated 2+ times with optional whitespace\n",
    "        text = re.sub(rf'(?:{escaped_phrase}\\s*){{{threshold},}}', phrase + ' ', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s{2,}', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: prepare for the text data\n",
    "load the Fake_pretraining_Texts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Hello! Contact us at support@data.org or call ...\n",
      "1    Hola! Este artículo está completamente en espa...\n",
      "2    <html><body><div><h1>Breaking News</h1><p>This...\n",
      "3    Buy now! Best product ever. Best product ever....\n",
      "4    Python 3.14 introduces several improvements in...\n",
      "5    Python 3.14 introduces several improvements in...\n",
      "6    <div>For inquiries, email jane_doe@example.com...\n",
      "7    Large Language Models are transforming the AI ...\n",
      "8                  这是一个包含有用技术信息的中文段落。电话号码：010-12345678\n",
      "9    Buy now! Best product ever. Best product ever....\n",
      "Name: Raw Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "fake_texts = pd.read_csv(\"test_data/data/Fake_Pretraining_Texts.csv\")\n",
    "raw_dataset = fake_texts[\"Raw Text\"]\n",
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Apply the Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! Contact us at support@data.org or call 123-456-7890. Your credit card 4111111111111111 was declined. This message is intended only for the recipient. Visit our site for more.',\n",
       " 'Breaking NewsThis is a major event!Contact us',\n",
       " 'Buy now! Best product ever. Best product ever. Best product ever.',\n",
       " 'Python 3.14 introduces several improvements including better error messages. Learn more on the official site.',\n",
       " 'Python 3.14 introduces several improvements including better error messages. Learn more on the official docs.',\n",
       " 'For inquiries, email jane_doe@example.com or visit our site. Card number: 378282246310005.',\n",
       " 'Large Language Models are transforming the AI landscape with few-shot capabilities.',\n",
       " 'Buy now! Best product ever. Best product ever. Best product ever.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Remove HTML + Language Filter\n",
    "step1 = clean_html_and_filter_lang(raw_dataset)\n",
    "display(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! Contact us at support@data.org or call 123-456-7890. Your credit card 4111111111111111 was declined. This message is intended only for the recipient. Visit our site for more.',\n",
       " 'Breaking NewsThis is a major event!Contact us',\n",
       " 'Buy now! Best product ever. Best product ever. Best product ever.',\n",
       " 'Python 3.14 introduces several improvements including better error messages. Learn more on the official site.',\n",
       " 'For inquiries, email jane_doe@example.com or visit our site. Card number: 378282246310005.',\n",
       " 'Large Language Models are transforming the AI landscape with few-shot capabilities.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Deduplicate Paragraphs\n",
    "step2 = minhash_deduplication(step1)\n",
    "display(step2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! Contact us at [EMAIL] or call [PHONE]. Your credit card [CREDIT_CARD] was declined. This message is intended only for the recipient. Visit our site for more.',\n",
       " 'Breaking NewsThis is a major event!Contact us',\n",
       " 'Buy now! Best product ever. Best product ever. Best product ever.',\n",
       " 'Python 3.14 introduces several improvements including better error messages. Learn more on the official site.',\n",
       " 'For inquiries, email [EMAIL] or visit our site. Card number: [CREDIT_CARD].',\n",
       " 'Large Language Models are transforming the AI landscape with few-shot capabilities.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Strip PII\n",
    "step3 = [strip_pii(t) for t in step2]\n",
    "display(step3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! Contact us at [EMAIL] or call [PHONE]. Your credit card [CREDIT_CARD] was declined. This message is intended only for the recipient. Visit our site for more.',\n",
       " 'Breaking NewsThis is a major event!Contact us',\n",
       " 'Buy now! Best product ever.',\n",
       " 'Python 3.14 introduces several improvements including better error messages. Learn more on the official site.',\n",
       " 'For inquiries, email [EMAIL] or visit our site. Card number: [CREDIT_CARD].',\n",
       " 'Large Language Models are transforming the AI landscape with few-shot capabilities.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Remove Repetitive N-grams\n",
    "cleaned_data = [remove_repetitive_ngrams(t) for t in step3]\n",
    "display(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned dataset sample:\n",
      "--- Article 1 ---\n",
      "Hello! Contact us at [EMAIL] or call [PHONE]. Your credit card [CREDIT_CARD] was declined. This message is intended only for the recipient. Visit our site for more.\n",
      "--- Article 2 ---\n",
      "Breaking NewsThis is a major event!Contact us\n",
      "--- Article 3 ---\n",
      "Buy now! Best product ever.\n",
      "--- Article 4 ---\n",
      "Python 3.14 introduces several improvements including better error messages. Learn more on the official site.\n",
      "--- Article 5 ---\n",
      "For inquiries, email [EMAIL] or visit our site. Card number: [CREDIT_CARD].\n",
      "--- Article 6 ---\n",
      "Large Language Models are transforming the AI landscape with few-shot capabilities.\n"
     ]
    }
   ],
   "source": [
    "# Done!\n",
    "print(\"✅ Cleaned dataset sample:\")\n",
    "for idx, text in enumerate(cleaned_data):\n",
    "    print(f\"--- Article {idx + 1} ---\")\n",
    "    print(text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
